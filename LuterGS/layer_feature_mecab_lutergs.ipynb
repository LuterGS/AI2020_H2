{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "layer_feature_mecab_lutergs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuterGS/AI2020_H2/blob/main/LuterGS/layer_feature_mecab_lutergs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbUtNDek4RVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eabf31a5-9670-42cb-9a64-200f36fffc3e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK7R3r404hWn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f68e8722-a711-47fa-85ab-5498831c9790"
      },
      "source": [
        "!pip install pytorch-crf\n",
        "!pip install seqeval==1.0.0\n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-crf\n",
            "  Downloading https://files.pythonhosted.org/packages/96/7d/4c4688e26ea015fc118a0327e5726e6596836abce9182d3738be8ec2e32a/pytorch_crf-0.7.2-py3-none-any.whl\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n",
            "Collecting seqeval==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/75/63/180f7556bfd9b0f89bc853ee46d01a3a611d74798230a930afac6873d15c/seqeval-1.0.0.tar.gz\n",
            "Collecting numpy==1.19.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/97/af8a92864a04bfa48f1b5c9b1f8bf2ccb2847f24530026f26dd223de4ca0/numpy-1.19.2-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5MB 219kB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.23.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (0.17.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.0.0-cp36-none-any.whl size=14022 sha256=62c405ec7c2fb6152c58b0e95dc4bfdff65f08372fdde9aad3ebbbe4dfeada93\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/82/18/6cfa15bb8d49855b0636bd72d12ab43fe9c4da8e8ba3b94db2\n",
            "Successfully built seqeval\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, threadpoolctl, scikit-learn, seqeval\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed numpy-1.19.2 scikit-learn-0.23.2 seqeval-1.0.0 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/21/9e2c0dbf9df856e6392a1aec1d18006c60b175aa4e31d351e8278a8a63c0/JPype1-1.2.0-cp36-cp36m-manylinux2010_x86_64.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 42.2MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.1MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.2)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, tweepy, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCkOg1kFD9Kj",
        "outputId": "29ef2dda-00aa-4a76-b1be-b071ab47b538"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab/\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 72 (delta 31), reused 20 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (72/72), done.\n",
            "/content/Mecab-ko-for-Google-Colab\n",
            "Installing konlpy.....\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.2)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.2.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2020-11-30 14:40:59--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db, 2406:da00:ff00::22c5:2ef4, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=5yUD%2B%2B8AXiUKltiZKHwcvi9nTi8%3D&Expires=1606748834&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2020-11-30 14:40:59--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=5yUD%2B%2B8AXiUKltiZKHwcvi9nTi8%3D&Expires=1606748834&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.38.36\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.38.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  2.72MB/s    in 0.5s    \n",
            "\n",
            "2020-11-30 14:41:00 (2.72 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2020-11-30 14:42:34--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c5:2ef4, 2406:da00:ff00::22c2:513, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=4mK9UkcjHZ3BkQYYx07LP08MujY%3D&Expires=1606748923&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2020-11-30 14:42:35--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=4mK9UkcjHZ3BkQYYx07LP08MujY%3D&Expires=1606748923&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.154.108\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.154.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  24.9MB/s    in 1.9s    \n",
            "\n",
            "2020-11-30 14:42:37 (24.9 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE78q61EW4s2"
      },
      "source": [
        "import os\n",
        "# from IPython.display import Image\n",
        "root_dir = \"/gdrive/MyDrive/colab/JHC\"\n",
        "# Image(os.path.join(root_dir, \"bi_GRU_crfs_model.PNG\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENjD9mNs5B3U"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Mecab\n",
        "\n",
        "def load_pos(config, f_name, tok, word2idx):\n",
        "    file = open(os.path.join(root_dir, f_name),'r',encoding='utf8')\n",
        "\n",
        "    feature = []\n",
        "    print(\"{} file loading...\".format(f_name))\n",
        "\n",
        "    for line in tqdm(file.readlines()):\n",
        "        try:\n",
        "            id, sentence, tags = line.strip().split('\\t')\n",
        "        except:\n",
        "            id, sentence = line.strip().split('\\t')\n",
        "        feat = get_pos(sentence, word2idx, tok, config[\"max_length\"])\n",
        "\n",
        "        feature.append(feat)\n",
        "\n",
        "    feature = torch.tensor(feature, dtype=torch.long)\n",
        "\n",
        "    return feature\n",
        "\n",
        "def get_pos(data, symbol2idx, tok, max_length = None):\n",
        "    feature = np.zeros(shape=(max_length), dtype=np.int)\n",
        "\n",
        "    line = ''.join(data.split()).replace(\"<SP>\", \" \")\n",
        "\n",
        "    for i, c in enumerate(line[:max_length]):\n",
        "        if c == ' ':\n",
        "            feature[i] = symbol2idx[\"<SP>\"]\n",
        "\n",
        "    if (tok == \"okt\"):\n",
        "        tokenizer = Okt()\n",
        "        line = tokenizer.pos(line)\n",
        "    elif (tok == \"mecab\"):\n",
        "        tokenizer = Mecab()\n",
        "        line = tokenizer.pos(line)\n",
        "    \n",
        "    index = 0\n",
        "    for word, pos in line:\n",
        "        if pos in symbol2idx.keys():\n",
        "            feat = symbol2idx[pos]\n",
        "        else:\n",
        "            feat = symbol2idx[\"<UNK>\"]\n",
        "\n",
        "        if (index >= max_length):\n",
        "            break\n",
        "        if (feature[index] == symbol2idx[\"<SP>\"]):\n",
        "            index+=1\n",
        "\n",
        "        for i in range(len(word)):\n",
        "            if (index < max_length):\n",
        "                feature[index] = feat\n",
        "                index+=1\n",
        "\n",
        "    return feature\n",
        "\n",
        "\n",
        "# 파라미터로 입력받은 파일에 저장된 단어 리스트를 딕셔너리 형태로 저장\n",
        "def load_vocab(f_name):\n",
        "    vocab_file = open(os.path.join(root_dir, f_name),'r',encoding='utf8')\n",
        "    print(\"{} vocab file loading...\".format(f_name))\n",
        "\n",
        "    # default 요소가 저장된 딕셔너리 생성\n",
        "    symbol2idx, idx2symbol = {\"<PAD>\":0, \"<UNK>\":1}, {0:\"<PAD>\", 1:\"<UNK>\"}\n",
        "\n",
        "    # 시작 인덱스 번호 저장\n",
        "    index = len(symbol2idx)\n",
        "    for line in tqdm(vocab_file.readlines()):\n",
        "        symbol = line.strip()\n",
        "        symbol2idx[symbol] = index\n",
        "        idx2symbol[index]= symbol\n",
        "        index+=1\n",
        "\n",
        "    return symbol2idx, idx2symbol\n",
        "\n",
        "# 입력 데이터를 고정 길이의 벡터로 표현하기 위한 함수\n",
        "def convert_data2feature(data, symbol2idx, max_length=None):\n",
        "    # 고정 길이의 0 벡터 생성\n",
        "    feature = np.zeros(shape=(max_length), dtype=np.int)\n",
        "    # 입력 문장을 공백 기준으로 split\n",
        "    words = data.split()\n",
        "\n",
        "    for idx, word in enumerate(words[:max_length]):\n",
        "        if word in symbol2idx.keys():\n",
        "            feature[idx] = symbol2idx[word]\n",
        "        else:\n",
        "            feature[idx] = symbol2idx[\"<UNK>\"]\n",
        "    return feature\n",
        "\n",
        "# 파라미터로 입력받은 파일로부터 tensor객체 생성\n",
        "def load_data(config, f_name, word2idx, tag2idx):\n",
        "    file = open(os.path.join(root_dir, f_name),'r',encoding='utf8')\n",
        "\n",
        "    # return할 문장/라벨 리스트 생성\n",
        "    indexing_inputs, indexing_tags = [], []\n",
        "\n",
        "    print(\"{} file loading...\".format(f_name))\n",
        "\n",
        "    # 실제 데이터는 아래와 같은 형태를 가짐\n",
        "    # 문장 \\t 태그\n",
        "    # 세 종 대 왕 은 <SP> 조 선 의 <SP> 4 대 <SP> 왕 이 야 \\t B_PS I_PS I_PS I_PS O <SP> B_LC I_LC O <SP> O O <SP> O O O\n",
        "    for line in tqdm(file.readlines()):\n",
        "        try:\n",
        "            id, sentence, tags = line.strip().split('\\t')\n",
        "        except:\n",
        "            id, sentence = line.strip().split('\\t')\n",
        "        input_sentence = convert_data2feature(sentence, word2idx, config[\"max_length\"])\n",
        "        indexing_tag = convert_data2feature(tags, tag2idx, config[\"max_length\"])\n",
        "\n",
        "        indexing_inputs.append(input_sentence)\n",
        "        indexing_tags.append(indexing_tag)\n",
        "    indexing_inputs = torch.tensor(indexing_inputs, dtype=torch.long)\n",
        "    indexing_tags = torch.tensor(indexing_tags, dtype=torch.long)\n",
        "\n",
        "    return indexing_inputs, indexing_tags\n",
        "\n",
        "# tensor 객체를 리스트 형으로 바꾸기 위한 함수\n",
        "def tensor2list(input_tensor):\n",
        "    return input_tensor.cpu().detach().numpy().tolist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5n-ucjSReR9"
      },
      "source": [
        "def train(config):\n",
        "    # 모델 객체 생성\n",
        "    model = RNN_CRF(config).cuda()\n",
        "    \n",
        "    # 모델을 학습하기위한 optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    accuracy_list = []\n",
        "    for epoch in range(config[\"epoch\"]):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # .cuda()를 이용하여 메모리에 업로드\n",
        "            batch = tuple(t.cuda() for t in batch)\n",
        "            input_features, pos_features, lookup_features, labels = batch\n",
        "\n",
        "            # loss 계산\n",
        "            loss = model.forward(input_features, pos_features, lookup_features, labels)\n",
        "\n",
        "            # 변화도 초기화\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # loss 값으로부터 모델 내부 각 매개변수에 대하여 gradient 계산\n",
        "            loss.backward()\n",
        "\n",
        "            # 모델 내부 각 매개변수 가중치 갱신\n",
        "            optimizer.step()\n",
        "\n",
        "            if (step + 1) % 50 == 0:\n",
        "                print(\"{} step processed.. current loss : {}\".format(step + 1, loss.data.item()))\n",
        "            losses.append(loss.data.item())\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Average Loss : {}\".format(np.mean(losses)))\n",
        "\n",
        "        # 모델 저장\n",
        "        torch.save(model.state_dict(), os.path.join(config[\"output_dir_path\"], \"epoch_{}.pt\".format(epoch + 1)))\n",
        "\n",
        "        do_test(model, test_dataloader, idx2tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBdtP9_4o96Z"
      },
      "source": [
        "def test(config):\n",
        "    # 모델 객체 생성\n",
        "    model = RNN_CRF(config).cuda()\n",
        "    # 저장된 가중치 Load\n",
        "    model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"trained_model_name\"])))\n",
        "    return do_test(model, test_dataloader, idx2tag)\n",
        "\n",
        "def do_test(model, test_dataloader, idx2tag):\n",
        "    print(test_dataloader)\n",
        "    result = []\n",
        "    model.eval()\n",
        "    predicts, answers = [], []\n",
        "    for step, batch in enumerate(test_dataloader):\n",
        "        # .cuda() 함수를 이용하요 메모리에 업로드\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "        # 데이터를 각 변수에 저장\n",
        "        input_features, pos, lookup, labels = batch\n",
        "\n",
        "        # 예측 라벨 출력\n",
        "        output = model(input_features, pos, lookup)\n",
        "        result.append(output)\n",
        "\n",
        "        # 성능 평가를 위해 예측 값과 정답 값 리스트에 저장\n",
        "        for idx, answer in enumerate(tensor2list(labels)):\n",
        "            answers.extend([idx2tag[e].replace(\"_\", \"-\") for e in answer if idx2tag[e] != \"<SP>\" and idx2tag[e] != \"<PAD>\"])\n",
        "            predicts.extend([idx2tag[e].replace(\"_\", \"-\") for i, e in enumerate(output[idx]) if idx2tag[answer[i]] != \"<SP>\" and idx2tag[answer[i]] != \"<PAD>\"] )\n",
        "    \n",
        "    # 성능 평가\n",
        "    print(classification_report(answers, predicts))\n",
        "\n",
        "    return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2hx2MgSajTE"
      },
      "source": [
        "config = {\"mode\": \"train\",\n",
        "              \"train_file\":\"ner_train.txt\",\n",
        "              \"dev_file\": \"ner_dev.txt\",\n",
        "              \"word_vocab_file\":\"vocab.txt\",\n",
        "              \"tag_vocab_file\":\"tag_vocab.txt\",\n",
        "              \"pos_vocab_file\":\"mecab_vocab.txt\",\n",
        "              \"pos_type\":\"mecab\",\n",
        "              \"trained_model_name\":\"74base.pt\",\n",
        "              \"output_dir_path\":os.path.join(root_dir, \"output\"),\n",
        "              \"word_vocab_size\":2160,\n",
        "              \"pos_vocab_size\":  46,\n",
        "              \"number_of_tags\": 14,\n",
        "              \"hidden_size\": 100,\n",
        "              \"dropout\":0.2,\n",
        "              \"num_filters\":100,\n",
        "              \"embedding_size\":100,\n",
        "              \"pos_embedding_size\": 8,\n",
        "              \"max_length\": 120,\n",
        "              \"batch_size\":64,\n",
        "              \"epoch\":20,\n",
        "              \"test_file\":\"test_dev.txt\"\n",
        "              }"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTmti9F9cX00",
        "outputId": "5f462366-6dc0-4a4e-9e2d-5cfbc263c1fd"
      },
      "source": [
        "from torch.utils.data import (DataLoader, TensorDataset)\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "word2idx, idx2word = load_vocab(config[\"word_vocab_file\"])\n",
        "tag2idx, idx2tag = load_vocab(config[\"tag_vocab_file\"])\n",
        "\n",
        "# 추가 feature 딕셔너리 생성\n",
        "pos2idx, idx2pos = load_vocab(config[\"pos_vocab_file\"])\n",
        "\n",
        "train_input_features, train_tags = load_data(config, config[\"train_file\"], word2idx, tag2idx)\n",
        "test_input_features, test_tags = load_data(config, config[\"dev_file\"], word2idx, tag2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2158/2158 [00:00<00:00, 399228.48it/s]\n",
            "100%|██████████| 12/12 [00:00<00:00, 61984.79it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 151518.37it/s]\n",
            " 26%|██▌       | 1890/7319 [00:00<00:00, 18895.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "vocab.txt vocab file loading...\n",
            "tag_vocab.txt vocab file loading...\n",
            "mecab_vocab.txt vocab file loading...\n",
            "ner_train.txt file loading...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7319/7319 [00:00<00:00, 17727.44it/s]\n",
            "100%|██████████| 995/995 [00:00<00:00, 20274.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ner_dev.txt file loading...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QECTmQFuvVcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1051da7-1142-4237-bafd-7648accd27d6"
      },
      "source": [
        "train_pos = load_pos(config, config[\"train_file\"], config[\"pos_type\"], pos2idx)\n",
        "test_pos = load_pos(config, config[\"dev_file\"], config[\"pos_type\"], pos2idx)\n",
        "# train_pos = np.load(os.path.join(root_dir, \"features\", \"train_mecab_pos.npy\"))\n",
        "# test_pos = np.load(os.path.join(root_dir, \"features\", \"test_mecab_pos.npy\"))\n",
        "\n",
        "train_pos = torch.tensor(train_pos)\n",
        "test_pos = torch.tensor(test_pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/7319 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ner_train.txt file loading...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7319/7319 [00:09<00:00, 765.77it/s]\n",
            "  8%|▊         | 79/995 [00:00<00:01, 780.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ner_dev.txt file loading...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 995/995 [00:01<00:00, 813.06it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKoiBAE9vXql"
      },
      "source": [
        "with open(os.path.join(root_dir, \"train_input\"), \"rb\") as f:\n",
        "    train_lookup = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(root_dir, \"test_input\"), \"rb\") as f:\n",
        "    test_lookup = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-vqH6X3PU60"
      },
      "source": [
        "with open(os.path.join(root_dir, \"train_dict_match.pickle\"), \"rb\") as f:\n",
        "    train_lookup_onehot = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(root_dir, \"test_dict_match.pickle\"), \"rb\") as f:\n",
        "    test_lookup_onehot = pickle.load(f)\n",
        "\n",
        "train_lookup_onehot = torch.tensor(train_lookup_onehot)\n",
        "test_lookup_onehot = torch.tensor(test_lookup_onehot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_lQ_1wuvZ9t"
      },
      "source": [
        "train_features = TensorDataset(train_input_features, train_pos, train_lookup_onehot, train_tags)\n",
        "train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
        "\n",
        "test_features = TensorDataset(test_input_features, test_pos, test_lookup_onehot, test_tags)\n",
        "test_dataloader = DataLoader(test_features, shuffle=False, batch_size=config[\"batch_size\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OBMYqR3AWJW"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "with open(os.path.join(root_dir,\"train_mecab_pos.npy\"), \"wb\") as f:\n",
        "    np.save(f, train_pos)\n",
        "\n",
        "with open(os.path.join(root_dir, \"test_mecab_pos.npy\"), \"wb\") as f:\n",
        "    np.save(f, test_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO278h6bhYCf"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchcrf import CRF\n",
        "\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "\n",
        "class RNN_CRF(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(RNN_CRF, self).__init__()\n",
        "        self.max_length = config[\"max_length\"]\n",
        "\n",
        "        # 전체 음절 개수\n",
        "        self.eumjeol_vocab_size = config[\"word_vocab_size\"]\n",
        "\n",
        "        # 음절 임베딩 사이즈\n",
        "        self.embedding_size = config[\"embedding_size\"]\n",
        "\n",
        "        self.pos_vocab_size = config[\"pos_vocab_size\"]\n",
        "        self.pos_embedding_size = config[\"pos_embedding_size\"]\n",
        "\n",
        "        # GRU 히든 사이즈\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "\n",
        "        # 분류할 태그의 개수\n",
        "        self.number_of_tags = config[\"number_of_tags\"]\n",
        "\n",
        "        self.num_filters = config[\"num_filters\"]\n",
        "\n",
        "        # 입력 데이터에 있는 각 음절 index를 대응하는 임베딩 벡터로 치환해주기 위한 임베딩 객체\n",
        "        self.embedding = nn.Embedding(num_embeddings=self.eumjeol_vocab_size,\n",
        "                                      embedding_dim=self.embedding_size,\n",
        "                                      padding_idx=0)\n",
        "        \n",
        "        self.pos_embedding = nn.Embedding(num_embeddings=self.pos_vocab_size,\n",
        "                                          embedding_dim=self.pos_embedding_size,\n",
        "                                          padding_idx=0)\n",
        "        \n",
        "        \n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        self._activation = nn.ReLU()\n",
        "\n",
        "        # Bi-GRU layer : 원본데이터\n",
        "        self.bi_gru = nn.GRU(input_size = self.embedding_size,\n",
        "                             hidden_size= self.hidden_size,\n",
        "                             num_layers=1,\n",
        "                             batch_first=True,\n",
        "                             bidirectional=True)\n",
        "        \n",
        "        # Bi-GRU layer : 첫번째 bi-gru 통과 > \n",
        "        self.second_bi_gru = nn.GRU(input_size = (self.hidden_size*2+self.pos_embedding_size+3),\n",
        "                             hidden_size = (self.hidden_size),\n",
        "                             num_layers=1,\n",
        "                             batch_first=True,\n",
        "                             bidirectional=True)\n",
        "    \n",
        "        \n",
        "        self.linear_layer = nn.Linear(in_features = (self.hidden_size*2), out_features = self.hidden_size)\n",
        "        \n",
        "\n",
        "        # CRF layer\n",
        "        self.crf = CRF(num_tags=self.number_of_tags, batch_first=True)\n",
        "\n",
        "        # fully_connected layer를 통하여 출력 크기를 number_of_tags에 맞춰줌\n",
        "        # (batch_size, max_length, hidden_size*2) -> (batch_size, max_length, number_of_tags)\n",
        "        self.hidden2num_tag = nn.Linear(in_features=(self.hidden_size*2), out_features=self.number_of_tags)\n",
        "\n",
        "    def forward(self, inputs, pos, lookup,labels=None):\n",
        "        # (batch_size, max_length) -> (batch_size, max_length, embedding_size)\n",
        "        eumjeol_inputs = self.embedding(inputs)\n",
        "        pos_inputs = self.pos_embedding(pos)\n",
        "\n",
        "        encoder_outputs, hidden_states = self.bi_gru(eumjeol_inputs)\n",
        "\n",
        "        first_encoder_outputs = torch.cat((encoder_outputs, pos_inputs, lookup),2)\n",
        "\n",
        "        second_encoder_outputs, _ = self.second_bi_gru(first_encoder_outputs)\n",
        "\n",
        "        # (batch_size, curr_max_length, hidden_size*2)\n",
        "        d_hidden_outputs = self.dropout(second_encoder_outputs)\n",
        "\n",
        "        # (batch_size, curr_max_length, hidden_size*2) -> (batch_size, curr_max_length, number_of_tags)\n",
        "        logits = self.hidden2num_tag(d_hidden_outputs)\n",
        "\n",
        "        if(labels is not None):\n",
        "            log_likelihood = self.crf(emissions=logits,\n",
        "                                      tags=labels,\n",
        "                                      reduction=\"mean\")\n",
        "\n",
        "            loss = log_likelihood * -1.0\n",
        "\n",
        "            return loss\n",
        "        else:\n",
        "            output = self.crf.decode(emissions=logits)\n",
        "\n",
        "            return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHrKeRDuDomd",
        "outputId": "69439eb2-42bd-4bb0-c581-b13a79e85f35"
      },
      "source": [
        "# train(config)\n",
        "# 모델 객체 생성\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f48bfd35128>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:45: UserWarning: <PAD> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.80      0.75      0.77       609\n",
            "          LC       0.79      0.70      0.74       534\n",
            "          OG       0.75      0.66      0.70       963\n",
            "          PS       0.76      0.73      0.74       733\n",
            "          TI       0.78      0.70      0.74        94\n",
            "\n",
            "   micro avg       0.77      0.70      0.73      2933\n",
            "   macro avg       0.78      0.71      0.74      2933\n",
            "weighted avg       0.77      0.70      0.74      2933\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQD7KCLdrM3p",
        "outputId": "ba0cc735-934b-4770-ff74-fd87ce7ac7b0"
      },
      "source": [
        "def get_result(filename):\n",
        "    result = test(config)\n",
        "\n",
        "    final_result = []\n",
        "    for datasets in result:\n",
        "        for single_data in datasets:\n",
        "            final_result.append(single_data)\n",
        "\n",
        "    test_file_reader = open(os.path.join(root_dir, config[\"dev_file\"]), \"r\", encoding=\"utf8\")\n",
        "    testfile_saver = []\n",
        "    for line in test_file_reader:\n",
        "        line = line.split(\"\\t\")\n",
        "        testfile_saver.append([line[0], line[1], len(line[1].split(\" \"))])\n",
        "    print(idx2tag)\n",
        "    print(final_result[0])\n",
        "\n",
        "\n",
        "    detailed_result = []\n",
        "    for i in range(len(final_result)):\n",
        "        one_data = []\n",
        "        for j in range(min(testfile_saver[i][2], config[\"max_length\"])):\n",
        "            one_data.append(idx2tag[final_result[i][j]])\n",
        "        detailed_result.append(one_data)\n",
        "\n",
        "    with open(os.path.join(root_dir, filename), \"w\", encoding=\"utf8\") as result_file:\n",
        "        for a in range(len(detailed_result)):\n",
        "            result_file.write(testfile_saver[a][0])\n",
        "            result_file.write(\"\\t\")\n",
        "            result_file.write(testfile_saver[a][1])\n",
        "            result_file.write(\"\\t\")\n",
        "            for b in range(len(detailed_result[a]) - 1):\n",
        "                result_file.write(detailed_result[a][b])\n",
        "                result_file.write(\" \")\n",
        "            result_file.write(detailed_result[a][-1])\n",
        "            result_file.write(\"\\n\")\n",
        "\n",
        "\n",
        "get_result(\"result_final_test.txt\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f48bfd35128>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:45: UserWarning: <PAD> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.80      0.75      0.77       609\n",
            "          LC       0.79      0.70      0.74       534\n",
            "          OG       0.75      0.66      0.70       963\n",
            "          PS       0.76      0.73      0.74       733\n",
            "          TI       0.78      0.70      0.74        94\n",
            "\n",
            "   micro avg       0.77      0.70      0.73      2933\n",
            "   macro avg       0.78      0.71      0.74      2933\n",
            "weighted avg       0.77      0.70      0.74      2933\n",
            "\n",
            "{0: '<PAD>', 1: '<UNK>', 2: '<SP>', 3: 'B_DT', 4: 'B_LC', 5: 'B_OG', 6: 'B_PS', 7: 'B_TI', 8: 'I_DT', 9: 'I_LC', 10: 'I_OG', 11: 'I_PS', 12: 'I_TI', 13: 'O'}\n",
            "[3, 8, 2, 13, 13, 13, 13, 13, 2, 13, 13, 13, 13, 13, 2, 13, 13, 13, 2, 13, 13, 13, 13, 13, 13, 13, 13, 2, 13, 13, 13, 2, 13, 13, 13, 2, 13, 13, 13, 13, 2, 3, 8, 2, 13, 13, 2, 13, 13, 13, 13, 2, 13, 13, 2, 13, 13, 13, 13, 13, 2, 5, 10, 10, 13, 2, 13, 13, 13, 13, 13, 2, 13, 13, 2, 5, 10, 10, 10, 2, 5, 10, 10, 10, 2, 13, 2, 13, 13, 2, 13, 13, 2, 13, 13, 13, 2, 13, 13, 2, 13, 2, 13, 13, 13, 2, 13, 13, 13, 2, 13, 13, 13, 2, 13, 13, 13, 13, 2, 13]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL_mAygNsVxD",
        "outputId": "89477d0b-808e-4912-fb7f-68171335eebe"
      },
      "source": [
        "# predict용 load_data\n",
        "def predict_load_data(config, f_name, word2idx, tag2idx):\n",
        "    file = open(os.path.join(root_dir, f_name),'r',encoding='utf8')\n",
        "\n",
        "    # return할 문장/라벨 리스트 생성\n",
        "    indexing_inputs, indexing_tags = [], []\n",
        "\n",
        "    print(\"{} file loading...\".format(f_name))\n",
        "\n",
        "    # 실제 데이터는 아래와 같은 형태를 가짐\n",
        "    # 문장 \\t 태그\n",
        "    # 세 종 대 왕 은 <SP> 조 선 의 <SP> 4 대 <SP> 왕 이 야 \\t B_PS I_PS I_PS I_PS O <SP> B_LC I_LC O <SP> O O <SP> O O O\n",
        "    for line in tqdm(file.readlines()):\n",
        "        try:\n",
        "            id, sentence, tags = line.strip().split('\\t')\n",
        "        except:\n",
        "            id, sentence = line.strip().split('\\t')\n",
        "        input_sentence = convert_data2feature(sentence, word2idx, config[\"max_length\"])\n",
        "        indexing_inputs.append(input_sentence)\n",
        "    indexing_inputs = torch.tensor(indexing_inputs, dtype=torch.long)\n",
        "\n",
        "    return indexing_inputs\n",
        "\n",
        "\n",
        "def get_result(filename):\n",
        "    model = RNN_CRF(config).cuda()\n",
        "\n",
        "    # test_lookup_onehot은 당일날에는 올라오는 데이터가 있어야함\n",
        "    predict_lookup_onehot = test_lookup_onehot\n",
        "\n",
        "    # 저장된 가중치 Load\n",
        "    model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"trained_model_name\"])))\n",
        "\n",
        "    # predict용 dataset 만들기\n",
        "    predict_pos = load_pos(config, config[\"test_file\"], config[\"pos_type\"], pos2idx)\n",
        "    predict_input_features = predict_load_data(config, config[\"test_file\"], word2idx, tag2idx)\n",
        "    predict_features = TensorDataset(predict_input_features, predict_pos, predict_lookup_onehot)\n",
        "    predict_dataloader = DataLoader(predict_features, shuffle=False, batch_size=config[\"batch_size\"])\n",
        "\n",
        "    model.eval()\n",
        "    predicts = []\n",
        "    for step, batch in enumerate(predict_dataloader):\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "        input_features, pos, lookup, = batch\n",
        "\n",
        "        output = model(input_features, pos, lookup)\n",
        "        predicts.append(output)\n",
        "\n",
        "    final_result = []\n",
        "    for datasets in predicts:\n",
        "        for single_data in datasets:\n",
        "            final_result.append(single_data)\n",
        "\n",
        "    test_file_reader = open(os.path.join(root_dir, config[\"test_file\"]), \"r\", encoding=\"utf8\")\n",
        "    testfile_saver = []\n",
        "    for line in test_file_reader:\n",
        "        line = line.split(\"\\t\")\n",
        "        testfile_saver.append([line[0], line[1], len(line[1].split(\" \"))])\n",
        "    print(idx2tag)\n",
        "    print(final_result[0])\n",
        "\n",
        "\n",
        "    detailed_result = []\n",
        "    for i in range(len(final_result)):\n",
        "        one_data = []\n",
        "        for j in range(min(testfile_saver[i][2], config[\"max_length\"])):\n",
        "            one_data.append(idx2tag[final_result[i][j]])\n",
        "        detailed_result.append(one_data)\n",
        "\n",
        "    with open(os.path.join(root_dir, filename), \"w\", encoding=\"utf8\") as result_file:\n",
        "        for a in range(len(detailed_result)):\n",
        "            result_file.write(testfile_saver[a][0])\n",
        "            result_file.write(\"\\t\")\n",
        "            result_file.write(testfile_saver[a][1])\n",
        "            result_file.write(\"\\t\")\n",
        "            for b in range(len(detailed_result[a]) - 1):\n",
        "                result_file.write(detailed_result[a][b])\n",
        "                result_file.write(\" \")\n",
        "            result_file.write(detailed_result[a][-1])\n",
        "            result_file.write(\"\\n\")\n",
        "\n",
        "\n",
        "get_result(\"result_final_test2.txt\")"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  8%|▊         | 83/995 [00:00<00:01, 824.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test_dev.txt file loading...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 995/995 [00:01<00:00, 815.03it/s]\n",
            "100%|██████████| 995/995 [00:00<00:00, 33042.75it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test_dev.txt file loading...\n",
            "{0: '<PAD>', 1: '<UNK>', 2: '<SP>', 3: 'B_DT', 4: 'B_LC', 5: 'B_OG', 6: 'B_PS', 7: 'B_TI', 8: 'I_DT', 9: 'I_LC', 10: 'I_OG', 11: 'I_PS', 12: 'I_TI', 13: 'O'}\n",
            "[3, 8, 2, 13, 13, 13, 13, 13, 2, 13, 13, 13, 13, 13, 2, 13, 13, 13, 2, 13, 13, 13, 13, 13, 13, 13, 13, 2, 13, 13, 13, 2, 13, 13, 13, 2, 13, 13, 13, 13, 2, 3, 8, 2, 13, 13, 2, 13, 13, 13, 13, 2, 13, 13, 2, 13, 13, 13, 13, 13, 2, 5, 10, 10, 13, 2, 13, 13, 13, 13, 13, 2, 13, 13, 2, 5, 10, 10, 10, 2, 5, 10, 10, 10, 2, 13, 2, 13, 13, 2, 13, 13, 2, 13, 13, 13, 2, 13, 13, 2, 13, 2, 13, 13, 13, 2, 13, 13, 13, 2, 13, 13, 13, 2, 13, 13, 13, 13, 2, 13]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XH-rM8Ewr_p",
        "outputId": "e3ec73f1-0022-471d-fe29-f7e059ffe9bf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: '<PAD>', 1: '<UNK>', 2: '<SP>', 3: 'B_DT', 4: 'B_LC', 5: 'B_OG', 6: 'B_PS', 7: 'B_TI', 8: 'I_DT', 9: 'I_LC', 10: 'I_OG', 11: 'I_PS', 12: 'I_TI', 13: 'O'}\n",
            "<UNK>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-iCgXzzu59n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}