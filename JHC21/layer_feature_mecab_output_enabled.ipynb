{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "layer_feature_mecab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuterGS/AI2020_H2/blob/main/JHC21/layer_feature_mecab_output_enabled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbUtNDek4RVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63baa08a-a497-4e11-ff64-f2f03a767c83"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK7R3r404hWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda68d58-e0c8-4ce8-b2ea-c0a726343ed3"
      },
      "source": [
        "!pip install pytorch-crf\n",
        "!pip install seqeval==1.0.0\n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.6/dist-packages (0.7.2)\n",
            "Requirement already satisfied: seqeval==1.0.0 in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy==1.19.2 in /usr/local/lib/python3.6/dist-packages (from seqeval==1.0.0) (1.19.2)\n",
            "Requirement already satisfied: scikit-learn==0.23.2 in /usr/local/lib/python3.6/dist-packages (from seqeval==1.0.0) (0.23.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (0.17.0)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.2)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCkOg1kFD9Kj",
        "outputId": "981942fd-d27d-431d-9b54-ec7fd0243d54"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab/\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Mecab-ko-for-Google-Colab' already exists and is not an empty directory.\n",
            "/content/Mecab-ko-for-Google-Colab\n",
            "Installing konlpy.....\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.2)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2020-11-27 09:19:00--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c0:3470, 2406:da00:ff00::22e9:9f55, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=1f8744xNUHEGVHDXU5ppWP7pQ7k%3D&Expires=1606470033&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2020-11-27 09:19:00--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=1f8744xNUHEGVHDXU5ppWP7pQ7k%3D&Expires=1606470033&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.68.172\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.68.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz.2’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-11-27 09:19:00 (51.9 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz.2’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2020-11-27 09:19:15--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 18.205.93.0, 18.205.93.2, 18.205.93.1, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|18.205.93.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=nRm1VhSoj3UvProSkdPgIVN9Bqo%3D&Expires=1606470048&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2020-11-27 09:19:15--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=nRm1VhSoj3UvProSkdPgIVN9Bqo%3D&Expires=1606470048&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.8.3\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.8.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz.2’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M   109MB/s    in 0.4s    \n",
            "\n",
            "2020-11-27 09:19:15 (109 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz.2’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE78q61EW4s2"
      },
      "source": [
        "import os\n",
        "# from IPython.display import Image\n",
        "root_dir = \"/gdrive/My Drive/Midterm\"\n",
        "# Image(os.path.join(root_dir, \"bi_GRU_crfs_model.PNG\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENjD9mNs5B3U"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Mecab\n",
        "\n",
        "def load_pos(config, f_name, tok, word2idx):\n",
        "    file = open(os.path.join(root_dir, f_name),'r',encoding='utf8')\n",
        "\n",
        "    feature = []\n",
        "    print(\"{} file loading...\".format(f_name))\n",
        "\n",
        "    for line in tqdm(file.readlines()):\n",
        "        try:\n",
        "            id, sentence, tags = line.strip().split('\\t')\n",
        "        except:\n",
        "            id, sentence = line.strip().split('\\t')\n",
        "        feat = get_pos(sentence, word2idx, tok, config[\"max_length\"])\n",
        "\n",
        "        feature.append(feat)\n",
        "\n",
        "    feature = torch.tensor(feature, dtype=torch.long)\n",
        "\n",
        "    return feature\n",
        "\n",
        "def get_pos(data, symbol2idx, tok, max_length = None):\n",
        "    feature = np.zeros(shape=(max_length), dtype=np.int)\n",
        "\n",
        "    line = ''.join(data.split()).replace(\"<SP>\", \" \")\n",
        "\n",
        "    for i, c in enumerate(line[:max_length]):\n",
        "        if c == ' ':\n",
        "            feature[i] = symbol2idx[\"<SP>\"]\n",
        "\n",
        "    if (tok == \"okt\"):\n",
        "        tokenizer = Okt()\n",
        "        line = tokenizer.pos(line)\n",
        "    elif (tok == \"mecab\"):\n",
        "        tokenizer = Mecab()\n",
        "        line = tokenizer.pos(line)\n",
        "    \n",
        "    index = 0\n",
        "    for word, pos in line:\n",
        "        if pos in symbol2idx.keys():\n",
        "            feat = symbol2idx[pos]\n",
        "        else:\n",
        "            feat = symbol2idx[\"<UNK>\"]\n",
        "\n",
        "        if (index >= max_length):\n",
        "            break\n",
        "        if (feature[index] == symbol2idx[\"<SP>\"]):\n",
        "            index+=1\n",
        "\n",
        "        for i in range(len(word)):\n",
        "            if (index < max_length):\n",
        "                feature[index] = feat\n",
        "                index+=1\n",
        "\n",
        "    return feature\n",
        "\n",
        "\n",
        "# 파라미터로 입력받은 파일에 저장된 단어 리스트를 딕셔너리 형태로 저장\n",
        "def load_vocab(f_name):\n",
        "    vocab_file = open(os.path.join(root_dir, f_name),'r',encoding='utf8')\n",
        "    print(\"{} vocab file loading...\".format(f_name))\n",
        "\n",
        "    # default 요소가 저장된 딕셔너리 생성\n",
        "    symbol2idx, idx2symbol = {\"<PAD>\":0, \"<UNK>\":1}, {0:\"<PAD>\", 1:\"<UNK>\"}\n",
        "\n",
        "    # 시작 인덱스 번호 저장\n",
        "    index = len(symbol2idx)\n",
        "    for line in tqdm(vocab_file.readlines()):\n",
        "        symbol = line.strip()\n",
        "        symbol2idx[symbol] = index\n",
        "        idx2symbol[index]= symbol\n",
        "        index+=1\n",
        "\n",
        "    return symbol2idx, idx2symbol\n",
        "\n",
        "# 입력 데이터를 고정 길이의 벡터로 표현하기 위한 함수\n",
        "def convert_data2feature(data, symbol2idx, max_length=None):\n",
        "    # 고정 길이의 0 벡터 생성\n",
        "    feature = np.zeros(shape=(max_length), dtype=np.int)\n",
        "    # 입력 문장을 공백 기준으로 split\n",
        "    words = data.split()\n",
        "\n",
        "    for idx, word in enumerate(words[:max_length]):\n",
        "        if word in symbol2idx.keys():\n",
        "            feature[idx] = symbol2idx[word]\n",
        "        else:\n",
        "            feature[idx] = symbol2idx[\"<UNK>\"]\n",
        "    return feature\n",
        "\n",
        "# 파라미터로 입력받은 파일로부터 tensor객체 생성\n",
        "def load_data(config, f_name, word2idx, tag2idx):\n",
        "    file = open(os.path.join(root_dir, f_name),'r',encoding='utf8')\n",
        "\n",
        "    # return할 문장/라벨 리스트 생성\n",
        "    indexing_inputs, indexing_tags = [], []\n",
        "\n",
        "    print(\"{} file loading...\".format(f_name))\n",
        "\n",
        "    # 실제 데이터는 아래와 같은 형태를 가짐\n",
        "    # 문장 \\t 태그\n",
        "    # 세 종 대 왕 은 <SP> 조 선 의 <SP> 4 대 <SP> 왕 이 야 \\t B_PS I_PS I_PS I_PS O <SP> B_LC I_LC O <SP> O O <SP> O O O\n",
        "    for line in tqdm(file.readlines()):\n",
        "        try:\n",
        "            id, sentence, tags = line.strip().split('\\t')\n",
        "        except:\n",
        "            id, sentence = line.strip().split('\\t')\n",
        "        input_sentence = convert_data2feature(sentence, word2idx, config[\"max_length\"])\n",
        "        indexing_tag = convert_data2feature(tags, tag2idx, config[\"max_length\"])\n",
        "\n",
        "        indexing_inputs.append(input_sentence)\n",
        "        indexing_tags.append(indexing_tag)\n",
        "    indexing_inputs = torch.tensor(indexing_inputs, dtype=torch.long)\n",
        "    indexing_tags = torch.tensor(indexing_tags, dtype=torch.long)\n",
        "\n",
        "    return indexing_inputs, indexing_tags\n",
        "\n",
        "# tensor 객체를 리스트 형으로 바꾸기 위한 함수\n",
        "def tensor2list(input_tensor):\n",
        "    return input_tensor.cpu().detach().numpy().tolist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5n-ucjSReR9"
      },
      "source": [
        "def train(config):\n",
        "    # 모델 객체 생성\n",
        "    model = RNN_CRF(config).cuda()\n",
        "    \n",
        "    # 모델을 학습하기위한 optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    accuracy_list = []\n",
        "    for epoch in range(config[\"epoch\"]):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # .cuda()를 이용하여 메모리에 업로드\n",
        "            batch = tuple(t.cuda() for t in batch)\n",
        "            input_features, pos_features, lookup_features, labels = batch\n",
        "\n",
        "            # loss 계산\n",
        "            loss = model.forward(input_features, pos_features, lookup_features, labels)\n",
        "\n",
        "            # 변화도 초기화\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # loss 값으로부터 모델 내부 각 매개변수에 대하여 gradient 계산\n",
        "            loss.backward()\n",
        "\n",
        "            # 모델 내부 각 매개변수 가중치 갱신\n",
        "            optimizer.step()\n",
        "\n",
        "            if (step + 1) % 50 == 0:\n",
        "                print(\"{} step processed.. current loss : {}\".format(step + 1, loss.data.item()))\n",
        "            losses.append(loss.data.item())\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Average Loss : {}\".format(np.mean(losses)))\n",
        "\n",
        "        # 모델 저장\n",
        "        torch.save(model.state_dict(), os.path.join(config[\"output_dir_path\"], \"epoch_{}.pt\".format(epoch + 1)))\n",
        "\n",
        "        do_test(model, test_dataloader, idx2tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBdtP9_4o96Z"
      },
      "source": [
        "def test(config):\n",
        "    # 모델 객체 생성\n",
        "    model = RNN_CRF(config).cuda()\n",
        "    # 단어 딕셔너리 생성\n",
        "    word2idx, idx2word = load_vocab(config[\"word_vocab_file\"])\n",
        "    tag2idx, idx2tag = load_vocab(config[\"tag_vocab_file\"])\n",
        "\n",
        "\n",
        "    # 저장된 가중치 Load\n",
        "    model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"trained_model_name\"])))\n",
        "\n",
        "    # 데이터 Load\n",
        "    test_input_features, test_tags = load_data(config, config[\"dev_file\"], word2idx, tag2idx)\n",
        "\n",
        "    # 불러온 데이터를 TensorDataset 객체로 변환\n",
        "    test_features = TensorDataset(test_input_features, test_tags)\n",
        "    test_dataloader = DataLoader(test_features, shuffle=False, batch_size=config[\"batch_size\"])\n",
        "    # 평가 함수 호출\n",
        "    do_test(model, test_dataloader, idx2tag)\n",
        "\n",
        "def do_test(model, test_dataloader, idx2tag):\n",
        "    model.eval()\n",
        "    predicts, answers = [], []\n",
        "    for step, batch in enumerate(test_dataloader):\n",
        "        # .cuda() 함수를 이용하요 메모리에 업로드\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "        # 데이터를 각 변수에 저장\n",
        "        input_features, pos, lookup, labels = batch\n",
        "\n",
        "        # 예측 라벨 출력\n",
        "        output = model(input_features, pos, lookup)\n",
        "\n",
        "        # 성능 평가를 위해 예측 값과 정답 값 리스트에 저장\n",
        "        for idx, answer in enumerate(tensor2list(labels)):\n",
        "            answers.extend([idx2tag[e].replace(\"_\", \"-\") for e in answer if idx2tag[e] != \"<SP>\" and idx2tag[e] != \"<PAD>\"])\n",
        "            predicts.extend([idx2tag[e].replace(\"_\", \"-\") for i, e in enumerate(output[idx]) if idx2tag[answer[i]] != \"<SP>\" and idx2tag[answer[i]] != \"<PAD>\"] )\n",
        "    \n",
        "    # 성능 평가\n",
        "    print(classification_report(answers, predicts))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2hx2MgSajTE"
      },
      "source": [
        "config = {\"mode\": \"train\",\n",
        "              \"train_file\":\"ner_train.txt\",\n",
        "              \"dev_file\": \"ner_dev.txt\",\n",
        "              \"word_vocab_file\":\"vocab.txt\",\n",
        "              \"tag_vocab_file\":\"tag_vocab.txt\",\n",
        "              \"pos_vocab_file\":\"mecab_vocab.txt\",\n",
        "              \"pos_type\":\"mecab\",\n",
        "              \"trained_model_name\":\"epoch_{}.pt\".format(5),\n",
        "              \"output_dir_path\":os.path.join(root_dir, \"output\"),\n",
        "              \"word_vocab_size\":2160,\n",
        "              \"pos_vocab_size\":  46,\n",
        "              \"number_of_tags\": 14,\n",
        "              \"hidden_size\": 100,\n",
        "              \"dropout\":0.2,\n",
        "              \"num_filters\":100,\n",
        "              \"embedding_size\":100,\n",
        "              \"pos_embedding_size\": 8,\n",
        "              \"max_length\": 120,\n",
        "              \"batch_size\":64,\n",
        "              \"epoch\":20,\n",
        "              }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTmti9F9cX00",
        "outputId": "ddcd2ee0-6ba6-4179-9236-24b1870c110c"
      },
      "source": [
        "from torch.utils.data import (DataLoader, TensorDataset)\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "word2idx, idx2word = load_vocab(config[\"word_vocab_file\"])\n",
        "tag2idx, idx2tag = load_vocab(config[\"tag_vocab_file\"])\n",
        "\n",
        "# 추가 feature 딕셔너리 생성\n",
        "pos2idx, idx2pos = load_vocab(config[\"pos_vocab_file\"])\n",
        "\n",
        "train_input_features, train_tags = load_data(config, config[\"train_file\"], word2idx, tag2idx)\n",
        "test_input_features, test_tags = load_data(config, config[\"dev_file\"], word2idx, tag2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2158/2158 [00:00<00:00, 802492.07it/s]\n",
            "100%|██████████| 12/12 [00:00<00:00, 38509.29it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 48147.50it/s]\n",
            " 31%|███▏      | 2296/7319 [00:00<00:00, 22957.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "vocab.txt vocab file loading...\n",
            "tag_vocab.txt vocab file loading...\n",
            "mecab_vocab.txt vocab file loading...\n",
            "ner_train.txt file loading...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7319/7319 [00:00<00:00, 22169.78it/s]\n",
            "100%|██████████| 995/995 [00:00<00:00, 22141.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ner_dev.txt file loading...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QECTmQFuvVcZ"
      },
      "source": [
        "# train_pos = load_pos(config, config[\"train_file\"], config[\"pos_type\"], pos2idx)\n",
        "# test_pos = load_pos(config, config[\"dev_file\"], config[\"pos_type\"], pos2idx)\n",
        "train_pos = np.load(os.path.join(root_dir, \"features\", \"train_mecab_pos.npy\"))\n",
        "test_pos = np.load(os.path.join(root_dir, \"features\", \"test_mecab_pos.npy\"))\n",
        "\n",
        "train_pos = torch.tensor(train_pos)\n",
        "test_pos = torch.tensor(test_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKoiBAE9vXql"
      },
      "source": [
        "with open(os.path.join(root_dir, \"features\", \"개체명사전 + 음절\", \"train_input\"), \"rb\") as f:\n",
        "    train_lookup = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(root_dir, \"features\", \"개체명사전 + 음절\", \"test_input\"), \"rb\") as f:\n",
        "    test_lookup = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-vqH6X3PU60"
      },
      "source": [
        "with open(os.path.join(root_dir, \"features\", \"개체명사전 lookup 원핫\", \"train_dict_match.pickle\"), \"rb\") as f:\n",
        "    train_lookup_onehot = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(root_dir, \"features\", \"개체명사전 lookup 원핫\", \"test_dict_match.pickle\"), \"rb\") as f:\n",
        "    test_lookup_onehot = pickle.load(f)\n",
        "\n",
        "train_lookup_onehot = torch.tensor(train_lookup_onehot)\n",
        "test_lookup_onehot = torch.tensor(test_lookup_onehot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_lQ_1wuvZ9t"
      },
      "source": [
        "train_features = TensorDataset(train_input_features, train_pos, train_lookup_onehot, train_tags)\n",
        "train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
        "\n",
        "test_features = TensorDataset(test_input_features, test_pos, test_lookup_onehot, test_tags)\n",
        "test_dataloader = DataLoader(test_features, shuffle=False, batch_size=config[\"batch_size\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OBMYqR3AWJW"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "with open(os.path.join(root_dir,\"train_mecab_pos.npy\"), \"wb\") as f:\n",
        "    np.save(f, train_pos)\n",
        "\n",
        "with open(os.path.join(root_dir, \"test_mecab_pos.npy\"), \"wb\") as f:\n",
        "    np.save(f, test_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO278h6bhYCf"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchcrf import CRF\n",
        "\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "\n",
        "class RNN_CRF(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(RNN_CRF, self).__init__()\n",
        "        self.max_length = config[\"max_length\"]\n",
        "\n",
        "        # 전체 음절 개수\n",
        "        self.eumjeol_vocab_size = config[\"word_vocab_size\"]\n",
        "\n",
        "        # 음절 임베딩 사이즈\n",
        "        self.embedding_size = config[\"embedding_size\"]\n",
        "\n",
        "        self.pos_vocab_size = config[\"pos_vocab_size\"]\n",
        "        self.pos_embedding_size = config[\"pos_embedding_size\"]\n",
        "\n",
        "        # GRU 히든 사이즈\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "\n",
        "        # 분류할 태그의 개수\n",
        "        self.number_of_tags = config[\"number_of_tags\"]\n",
        "\n",
        "        self.num_filters = config[\"num_filters\"]\n",
        "\n",
        "        # 입력 데이터에 있는 각 음절 index를 대응하는 임베딩 벡터로 치환해주기 위한 임베딩 객체\n",
        "        self.embedding = nn.Embedding(num_embeddings=self.eumjeol_vocab_size,\n",
        "                                      embedding_dim=self.embedding_size,\n",
        "                                      padding_idx=0)\n",
        "        \n",
        "        self.pos_embedding = nn.Embedding(num_embeddings=self.pos_vocab_size,\n",
        "                                          embedding_dim=self.pos_embedding_size,\n",
        "                                          padding_idx=0)\n",
        "        \n",
        "        \n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        self._activation = nn.ReLU()\n",
        "\n",
        "        # Bi-GRU layer : 원본데이터\n",
        "        self.bi_gru = nn.GRU(input_size = self.embedding_size,\n",
        "                             hidden_size= self.hidden_size,\n",
        "                             num_layers=1,\n",
        "                             batch_first=True,\n",
        "                             bidirectional=True)\n",
        "        \n",
        "        # Bi-GRU layer : 첫번째 bi-gru 통과 > \n",
        "        self.second_bi_gru = nn.GRU(input_size = (self.hidden_size*2+self.pos_embedding_size+3),\n",
        "                             hidden_size = (self.hidden_size),\n",
        "                             num_layers=1,\n",
        "                             batch_first=True,\n",
        "                             bidirectional=True)\n",
        "    \n",
        "        \n",
        "        self.linear_layer = nn.Linear(in_features = (self.hidden_size*2), out_features = self.hidden_size)\n",
        "        \n",
        "\n",
        "        # CRF layer\n",
        "        self.crf = CRF(num_tags=self.number_of_tags, batch_first=True)\n",
        "\n",
        "        # fully_connected layer를 통하여 출력 크기를 number_of_tags에 맞춰줌\n",
        "        # (batch_size, max_length, hidden_size*2) -> (batch_size, max_length, number_of_tags)\n",
        "        self.hidden2num_tag = nn.Linear(in_features=(self.hidden_size*2), out_features=self.number_of_tags)\n",
        "\n",
        "    def forward(self, inputs, pos, lookup,labels=None):\n",
        "        # (batch_size, max_length) -> (batch_size, max_length, embedding_size)\n",
        "        eumjeol_inputs = self.embedding(inputs)\n",
        "        pos_inputs = self.pos_embedding(pos)\n",
        "\n",
        "        encoder_outputs, hidden_states = self.bi_gru(eumjeol_inputs)\n",
        "\n",
        "        first_encoder_outputs = torch.cat((encoder_outputs, pos_inputs, lookup),2)\n",
        "\n",
        "        second_encoder_outputs, _ = self.second_bi_gru(first_encoder_outputs)\n",
        "\n",
        "        # (batch_size, curr_max_length, hidden_size*2)\n",
        "        d_hidden_outputs = self.dropout(second_encoder_outputs)\n",
        "\n",
        "        # (batch_size, curr_max_length, hidden_size*2) -> (batch_size, curr_max_length, number_of_tags)\n",
        "        logits = self.hidden2num_tag(d_hidden_outputs)\n",
        "\n",
        "        if(labels is not None):\n",
        "            log_likelihood = self.crf(emissions=logits,\n",
        "                                      tags=labels,\n",
        "                                      reduction=\"mean\")\n",
        "\n",
        "            loss = log_likelihood * -1.0\n",
        "\n",
        "            return loss\n",
        "        else:\n",
        "            output = self.crf.decode(emissions=logits)\n",
        "\n",
        "            return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHrKeRDuDomd",
        "outputId": "53eb7f3b-d157-4afe-84d5-e17293651832"
      },
      "source": [
        "train(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50 step processed.. current loss : 16.812713623046875\n",
            "100 step processed.. current loss : 13.082791328430176\n",
            "Average Loss : 26.80925692682681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:45: UserWarning: <PAD> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.61      0.65      0.63       609\n",
            "          LC       0.72      0.58      0.65       534\n",
            "          OG       0.62      0.31      0.41       963\n",
            "          PS       0.52      0.66      0.59       733\n",
            "          TI       0.42      0.32      0.36        94\n",
            "\n",
            "   micro avg       0.59      0.52      0.55      2933\n",
            "   macro avg       0.58      0.50      0.53      2933\n",
            "weighted avg       0.61      0.52      0.54      2933\n",
            "\n",
            "50 step processed.. current loss : 9.531084060668945\n",
            "100 step processed.. current loss : 7.959637641906738\n",
            "Average Loss : 8.825541861160941\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.78      0.73      0.75       609\n",
            "          LC       0.70      0.66      0.68       534\n",
            "          OG       0.68      0.55      0.61       963\n",
            "          PS       0.82      0.64      0.72       733\n",
            "          TI       0.76      0.64      0.69        94\n",
            "\n",
            "   micro avg       0.74      0.63      0.68      2933\n",
            "   macro avg       0.75      0.64      0.69      2933\n",
            "weighted avg       0.74      0.63      0.68      2933\n",
            "\n",
            "50 step processed.. current loss : 7.559073448181152\n",
            "100 step processed.. current loss : 6.740299224853516\n",
            "Average Loss : 6.034325918943986\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.78      0.75      0.76       609\n",
            "          LC       0.71      0.70      0.70       534\n",
            "          OG       0.71      0.58      0.64       963\n",
            "          PS       0.81      0.67      0.73       733\n",
            "          TI       0.78      0.64      0.70        94\n",
            "\n",
            "   micro avg       0.75      0.66      0.70      2933\n",
            "   macro avg       0.76      0.67      0.71      2933\n",
            "weighted avg       0.75      0.66      0.70      2933\n",
            "\n",
            "50 step processed.. current loss : 3.4644813537597656\n",
            "100 step processed.. current loss : 4.3070783615112305\n",
            "Average Loss : 4.344351443000462\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.77      0.73      0.75       609\n",
            "          LC       0.74      0.70      0.72       534\n",
            "          OG       0.72      0.62      0.67       963\n",
            "          PS       0.80      0.68      0.74       733\n",
            "          TI       0.78      0.69      0.73        94\n",
            "\n",
            "   micro avg       0.76      0.67      0.71      2933\n",
            "   macro avg       0.76      0.68      0.72      2933\n",
            "weighted avg       0.76      0.67      0.71      2933\n",
            "\n",
            "50 step processed.. current loss : 3.41304874420166\n",
            "100 step processed.. current loss : 3.073352813720703\n",
            "Average Loss : 3.245358842352162\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.81      0.73      0.77       609\n",
            "          LC       0.72      0.71      0.72       534\n",
            "          OG       0.74      0.59      0.66       963\n",
            "          PS       0.80      0.70      0.74       733\n",
            "          TI       0.80      0.70      0.75        94\n",
            "\n",
            "   micro avg       0.77      0.67      0.72      2933\n",
            "   macro avg       0.78      0.69      0.73      2933\n",
            "weighted avg       0.77      0.67      0.72      2933\n",
            "\n",
            "50 step processed.. current loss : 2.3580760955810547\n",
            "100 step processed.. current loss : 2.4561901092529297\n",
            "Average Loss : 2.4609103824781333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.81      0.76      0.78       609\n",
            "          LC       0.77      0.67      0.71       534\n",
            "          OG       0.67      0.66      0.67       963\n",
            "          PS       0.77      0.72      0.74       733\n",
            "          TI       0.80      0.72      0.76        94\n",
            "\n",
            "   micro avg       0.74      0.70      0.72      2933\n",
            "   macro avg       0.76      0.71      0.73      2933\n",
            "weighted avg       0.74      0.70      0.72      2933\n",
            "\n",
            "50 step processed.. current loss : 1.7785415649414062\n",
            "100 step processed.. current loss : 2.1639328002929688\n",
            "Average Loss : 1.898155985707822\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.80      0.75      0.77       609\n",
            "          LC       0.75      0.68      0.71       534\n",
            "          OG       0.74      0.58      0.65       963\n",
            "          PS       0.81      0.73      0.77       733\n",
            "          TI       0.78      0.69      0.73        94\n",
            "\n",
            "   micro avg       0.77      0.67      0.72      2933\n",
            "   macro avg       0.78      0.68      0.73      2933\n",
            "weighted avg       0.77      0.67      0.72      2933\n",
            "\n",
            "50 step processed.. current loss : 1.3173952102661133\n",
            "100 step processed.. current loss : 1.9529342651367188\n",
            "Average Loss : 1.5772737357927404\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.82      0.76      0.79       609\n",
            "          LC       0.77      0.71      0.74       534\n",
            "          OG       0.71      0.66      0.68       963\n",
            "          PS       0.79      0.71      0.75       733\n",
            "          TI       0.76      0.66      0.70        94\n",
            "\n",
            "   micro avg       0.76      0.70      0.73      2933\n",
            "   macro avg       0.77      0.70      0.73      2933\n",
            "weighted avg       0.76      0.70      0.73      2933\n",
            "\n",
            "50 step processed.. current loss : 0.7618007659912109\n",
            "100 step processed.. current loss : 1.2692136764526367\n",
            "Average Loss : 1.2887363423471865\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.80      0.73      0.76       609\n",
            "          LC       0.75      0.67      0.71       534\n",
            "          OG       0.72      0.66      0.69       963\n",
            "          PS       0.78      0.72      0.75       733\n",
            "          TI       0.79      0.68      0.73        94\n",
            "\n",
            "   micro avg       0.76      0.69      0.72      2933\n",
            "   macro avg       0.77      0.69      0.73      2933\n",
            "weighted avg       0.76      0.69      0.72      2933\n",
            "\n",
            "50 step processed.. current loss : 1.075094223022461\n",
            "100 step processed.. current loss : 0.9882535934448242\n",
            "Average Loss : 1.1197677042173302\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.74      0.75      0.75       609\n",
            "          LC       0.75      0.71      0.73       534\n",
            "          OG       0.66      0.66      0.66       963\n",
            "          PS       0.76      0.73      0.74       733\n",
            "          TI       0.82      0.71      0.76        94\n",
            "\n",
            "   micro avg       0.72      0.71      0.71      2933\n",
            "   macro avg       0.75      0.71      0.73      2933\n",
            "weighted avg       0.72      0.71      0.72      2933\n",
            "\n",
            "50 step processed.. current loss : 1.0787124633789062\n",
            "100 step processed.. current loss : 1.1666450500488281\n",
            "Average Loss : 0.9613720235617265\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.77      0.73      0.75       609\n",
            "          LC       0.71      0.74      0.73       534\n",
            "          OG       0.77      0.56      0.65       963\n",
            "          PS       0.75      0.73      0.74       733\n",
            "          TI       0.77      0.63      0.69        94\n",
            "\n",
            "   micro avg       0.75      0.68      0.71      2933\n",
            "   macro avg       0.75      0.68      0.71      2933\n",
            "weighted avg       0.75      0.68      0.71      2933\n",
            "\n",
            "50 step processed.. current loss : 0.4996604919433594\n",
            "100 step processed.. current loss : 0.8762092590332031\n",
            "Average Loss : 0.9473662905071093\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.77      0.73      0.75       609\n",
            "          LC       0.74      0.72      0.73       534\n",
            "          OG       0.69      0.64      0.66       963\n",
            "          PS       0.76      0.71      0.74       733\n",
            "          TI       0.74      0.72      0.73        94\n",
            "\n",
            "   micro avg       0.74      0.69      0.71      2933\n",
            "   macro avg       0.74      0.71      0.72      2933\n",
            "weighted avg       0.74      0.69      0.71      2933\n",
            "\n",
            "50 step processed.. current loss : 1.18914794921875\n",
            "100 step processed.. current loss : 0.7936534881591797\n",
            "Average Loss : 0.8922786878502887\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.80      0.76      0.78       609\n",
            "          LC       0.75      0.70      0.72       534\n",
            "          OG       0.65      0.67      0.66       963\n",
            "          PS       0.75      0.72      0.73       733\n",
            "          TI       0.74      0.72      0.73        94\n",
            "\n",
            "   micro avg       0.72      0.71      0.72      2933\n",
            "   macro avg       0.74      0.71      0.73      2933\n",
            "weighted avg       0.73      0.71      0.72      2933\n",
            "\n",
            "50 step processed.. current loss : 0.8645420074462891\n",
            "100 step processed.. current loss : 0.7817974090576172\n",
            "Average Loss : 0.8243827933850496\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.80      0.75      0.77       609\n",
            "          LC       0.76      0.74      0.75       534\n",
            "          OG       0.68      0.65      0.67       963\n",
            "          PS       0.76      0.71      0.73       733\n",
            "          TI       0.78      0.76      0.77        94\n",
            "\n",
            "   micro avg       0.74      0.71      0.72      2933\n",
            "   macro avg       0.76      0.72      0.74      2933\n",
            "weighted avg       0.74      0.71      0.72      2933\n",
            "\n",
            "50 step processed.. current loss : 0.6069526672363281\n",
            "100 step processed.. current loss : 1.150456428527832\n",
            "Average Loss : 0.7523265988930412\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.76      0.73      0.74       609\n",
            "          LC       0.74      0.71      0.72       534\n",
            "          OG       0.70      0.63      0.67       963\n",
            "          PS       0.80      0.68      0.73       733\n",
            "          TI       0.78      0.71      0.74        94\n",
            "\n",
            "   micro avg       0.75      0.68      0.71      2933\n",
            "   macro avg       0.76      0.69      0.72      2933\n",
            "weighted avg       0.75      0.68      0.71      2933\n",
            "\n",
            "50 step processed.. current loss : 1.1412687301635742\n",
            "100 step processed.. current loss : 0.8634424209594727\n",
            "Average Loss : 0.7799434197985607\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.78      0.72      0.75       609\n",
            "          LC       0.73      0.68      0.71       534\n",
            "          OG       0.63      0.68      0.66       963\n",
            "          PS       0.73      0.73      0.73       733\n",
            "          TI       0.70      0.63      0.66        94\n",
            "\n",
            "   micro avg       0.70      0.70      0.70      2933\n",
            "   macro avg       0.71      0.69      0.70      2933\n",
            "weighted avg       0.71      0.70      0.70      2933\n",
            "\n",
            "50 step processed.. current loss : 0.8972358703613281\n",
            "100 step processed.. current loss : 0.7763643264770508\n",
            "Average Loss : 0.9086480254712312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:45: UserWarning: <SP> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.79      0.71      0.75       609\n",
            "          LC       0.74      0.70      0.72       534\n",
            "          OG       0.73      0.61      0.67       963\n",
            "          PS       0.79      0.66      0.72       733\n",
            "          TI       0.78      0.65      0.71        94\n",
            "\n",
            "   micro avg       0.76      0.66      0.71      2933\n",
            "   macro avg       0.77      0.67      0.71      2933\n",
            "weighted avg       0.76      0.66      0.71      2933\n",
            "\n",
            "50 step processed.. current loss : 0.602747917175293\n",
            "100 step processed.. current loss : 0.8627939224243164\n",
            "Average Loss : 0.7997348951256793\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.79      0.75      0.77       609\n",
            "          LC       0.77      0.66      0.71       534\n",
            "          OG       0.71      0.64      0.67       963\n",
            "          PS       0.72      0.74      0.73       733\n",
            "          TI       0.76      0.64      0.69        94\n",
            "\n",
            "   micro avg       0.74      0.69      0.72      2933\n",
            "   macro avg       0.75      0.69      0.72      2933\n",
            "weighted avg       0.74      0.69      0.72      2933\n",
            "\n",
            "50 step processed.. current loss : 0.9006538391113281\n",
            "100 step processed.. current loss : 0.7789993286132812\n",
            "Average Loss : 0.7967159659966179\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.78      0.73      0.76       609\n",
            "          LC       0.69      0.73      0.71       534\n",
            "          OG       0.76      0.55      0.64       963\n",
            "          PS       0.80      0.72      0.75       733\n",
            "          TI       0.75      0.66      0.70        94\n",
            "\n",
            "   micro avg       0.76      0.66      0.71      2933\n",
            "   macro avg       0.75      0.68      0.71      2933\n",
            "weighted avg       0.76      0.66      0.71      2933\n",
            "\n",
            "50 step processed.. current loss : 0.5903730392456055\n",
            "100 step processed.. current loss : 0.9966888427734375\n",
            "Average Loss : 0.7085290623747784\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.75      0.72      0.73       609\n",
            "          LC       0.75      0.71      0.73       534\n",
            "          OG       0.71      0.63      0.67       963\n",
            "          PS       0.77      0.72      0.74       733\n",
            "          TI       0.76      0.71      0.74        94\n",
            "\n",
            "   micro avg       0.74      0.69      0.71      2933\n",
            "   macro avg       0.75      0.70      0.72      2933\n",
            "weighted avg       0.74      0.69      0.71      2933\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmo283fQzz3Q"
      },
      "source": [
        "# predict용 load_data\n",
        "def predict_load_data(config, f_name, word2idx, tag2idx):\n",
        "    file = open(os.path.join(root_dir, f_name),'r',encoding='utf8')\n",
        "\n",
        "    # return할 문장/라벨 리스트 생성\n",
        "    indexing_inputs, indexing_tags = [], []\n",
        "\n",
        "    print(\"{} file loading...\".format(f_name))\n",
        "\n",
        "    # 실제 데이터는 아래와 같은 형태를 가짐\n",
        "    # 문장 \\t 태그\n",
        "    # 세 종 대 왕 은 <SP> 조 선 의 <SP> 4 대 <SP> 왕 이 야 \\t B_PS I_PS I_PS I_PS O <SP> B_LC I_LC O <SP> O O <SP> O O O\n",
        "    for line in tqdm(file.readlines()):\n",
        "        try:\n",
        "            id, sentence, tags = line.strip().split('\\t')\n",
        "        except:\n",
        "            id, sentence = line.strip().split('\\t')\n",
        "        input_sentence = convert_data2feature(sentence, word2idx, config[\"max_length\"])\n",
        "        indexing_inputs.append(input_sentence)\n",
        "    indexing_inputs = torch.tensor(indexing_inputs, dtype=torch.long)\n",
        "\n",
        "    return indexing_inputs\n",
        "\n",
        "\n",
        "def get_result(filename):\n",
        "    model = RNN_CRF(config).cuda()\n",
        "\n",
        "    # test_lookup_onehot은 당일날에는 올라오는 데이터가 있어야함\n",
        "    predict_lookup_onehot = test_lookup_onehot\n",
        "\n",
        "    # 저장된 가중치 Load\n",
        "    model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"trained_model_name\"])))\n",
        "\n",
        "    # predict용 dataset 만들기\n",
        "    predict_pos = load_pos(config, config[\"test_file\"], config[\"pos_type\"], pos2idx)\n",
        "    predict_input_features = predict_load_data(config, config[\"test_file\"], word2idx, tag2idx)\n",
        "    predict_features = TensorDataset(predict_input_features, predict_pos, predict_lookup_onehot)\n",
        "    predict_dataloader = DataLoader(predict_features, shuffle=False, batch_size=config[\"batch_size\"])\n",
        "\n",
        "    model.eval()\n",
        "    predicts = []\n",
        "    for step, batch in enumerate(predict_dataloader):\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "        input_features, pos, lookup, = batch\n",
        "\n",
        "        output = model(input_features, pos, lookup)\n",
        "        predicts.append(output)\n",
        "\n",
        "    final_result = []\n",
        "    for datasets in predicts:\n",
        "        for single_data in datasets:\n",
        "            final_result.append(single_data)\n",
        "\n",
        "    test_file_reader = open(os.path.join(root_dir, config[\"test_file\"]), \"r\", encoding=\"utf8\")\n",
        "    testfile_saver = []\n",
        "    for line in test_file_reader:\n",
        "        line = line.split(\"\\t\")\n",
        "        testfile_saver.append([line[0], line[1], len(line[1].split(\" \"))])\n",
        "    print(idx2tag)\n",
        "    print(final_result[0])\n",
        "\n",
        "\n",
        "    detailed_result = []\n",
        "    for i in range(len(final_result)):\n",
        "        one_data = []\n",
        "        for j in range(min(testfile_saver[i][2], config[\"max_length\"])):\n",
        "            one_data.append(idx2tag[final_result[i][j]])\n",
        "        detailed_result.append(one_data)\n",
        "\n",
        "    with open(os.path.join(root_dir, filename), \"w\", encoding=\"utf8\") as result_file:\n",
        "        for a in range(len(detailed_result)):\n",
        "            result_file.write(testfile_saver[a][0])\n",
        "            result_file.write(\"\\t\")\n",
        "            result_file.write(testfile_saver[a][1])\n",
        "            result_file.write(\"\\t\")\n",
        "            for b in range(len(detailed_result[a]) - 1):\n",
        "                result_file.write(detailed_result[a][b])\n",
        "                result_file.write(\" \")\n",
        "            result_file.write(detailed_result[a][-1])\n",
        "            result_file.write(\"\\n\")\n",
        "\n",
        "\n",
        "get_result(\"result_final_test2.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}